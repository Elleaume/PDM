{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc93588",
   "metadata": {},
   "source": [
    "## Test Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a57bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nibabel as nib\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "from decoder_pretrain import DecoderPretrainNet\n",
    "from encoder_pretrain import EncoderPretrainNet\n",
    "from gloss_dminus import GlobalLossDminus\n",
    "from gloss_d import GlobalLossD\n",
    "from demeaned_gloss_d import DemeanedGlobalLossD\n",
    "from pretraining_utils import *\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40b220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "with open('configs/preprocessing_datasets.json') as config_file:\n",
    "    config_datasets = json.load(config_file)\n",
    "with open('configs/config_encoder-Copy1.json') as config_file:\n",
    "    config_encoder = json.load(config_file)\n",
    "\n",
    "# init W&B\n",
    "#print(\"Using W&B in %s mode\" % 'online')\n",
    "#wandb.init(project=config_encoder[\"model\"], mode='online')\n",
    "seed = config_encoder['seed']\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# choose model from config\n",
    "model = EncoderPretrainNet(config_encoder)\n",
    "\n",
    "# choose specified optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config_encoder[\"lr\"])\n",
    "#criterion = {\"global_dminus\": lambda: GlobalLossDminus(config_encoder),\n",
    "#             \"global_d\": lambda: GlobalLossD(config_encoder),\n",
    "#             \"representation_loss_global_d\" : lambda : GlobalLossD(config_encoder),\n",
    "#             \"2_steps_global_d\": lambda : GlobalLossD(config_encoder),\n",
    "#             \"demeaned_representation_loss\": lambda : GlobalLossD(config_encoder),\n",
    "             #\"demeaned_representation_loss_2\": lambda : GlobalLossD(config_encoder)\n",
    "#            }[config_encoder['loss']]()\n",
    "\n",
    "criterion = GlobalLossD(config_encoder)\n",
    "\n",
    "if config_encoder['loss'] == '2_steps_global_d' :\n",
    "    criterion2 = GlobalLossD(config_encoder, within_dataset = True)\n",
    "if config_encoder['loss'] == 'demeaned_representation_loss' :\n",
    "    criterion2 = DemeanedGlobalLossD(config_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dca5b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n",
      "Running model encoder_pretrain\n",
      "Parameters used : \n",
      "Full traing dataset, no validation set\n",
      "loss :  demeaned_representation_loss_2\n",
      "weight_loss :  1\n",
      "n_volumes :  3\n",
      "n_transforms :  2\n",
      "n_datasets :  4\n",
      "max_epochs :  2\n",
      "batch size :  48\n",
      "temp_fac :  1\n",
      "save folder :  ./trained_models/Cardiac_only/pretraining_FullTrainSet/demeaned_representation_loss_2teeeest/4datasets_3volumesPerBatch_2transforms_tempfac1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)\n",
    "model.to(device)\n",
    "print(\"Running model %s\" % config_encoder[\"model\"])\n",
    "\n",
    "n_parts = config_encoder['n_parts']\n",
    "n_datasets = config_encoder['n_datasets']\n",
    "n_volumes = config_encoder['n_volumes']\n",
    "n_transforms = config_encoder['n_transforms']\n",
    "resize_size = config_encoder['resize_size']\n",
    "n_channels = config_encoder['n_channels']\n",
    "loss_pretraining = config_encoder['loss'] \n",
    "lambda_ = config_encoder['lambda']\n",
    "batch_size = n_parts * n_datasets * n_volumes\n",
    "perp_val = 80\n",
    "max_epochs = 2#config_encoder['max_epochs']\n",
    "weight_loss = config_encoder['weight_loss']\n",
    "save_global_path = config_encoder['save_global_path']\n",
    "temp_fac = config_encoder[\"temp_fac\"]\n",
    "\n",
    "date = str(time.strftime(\"%Y%h%d_%Hh%M\"))\n",
    "if loss_pretraining == 'representation_loss' or loss_pretraining == '2_steps_global_d':\n",
    "    save_directory = save_global_path + loss_pretraining + 'teeeest/' + str(n_datasets) +'datasets_' + str(n_volumes) \\\n",
    "                    + 'volumesPerBatch_' + str(n_transforms) + 'transforms'+'_lb' + str(lambda_)+'_tempfac' + str(temp_fac)\n",
    "else :\n",
    "    save_directory = save_global_path + loss_pretraining + 'teeeest/' + str(n_datasets) +'datasets_' + str(n_volumes) \\\n",
    "                    + 'volumesPerBatch_' + str(n_transforms) + 'transforms_tempfac' + str(temp_fac)\n",
    "save_models = save_directory + '/save_models/'\n",
    "Path(save_models).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Parameters used : ')\n",
    "print('Full traing dataset, no validation set')\n",
    "print('loss : ', loss_pretraining)\n",
    "print('weight_loss : ', weight_loss)\n",
    "print('n_volumes : ', n_volumes)\n",
    "print('n_transforms : ', n_transforms)\n",
    "print('n_datasets : ', n_datasets)\n",
    "print('max_epochs : ', max_epochs)\n",
    "print('batch size : ', batch_size)\n",
    "print('temp_fac : ', temp_fac)\n",
    "print('save folder : ', save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddaa856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainDatasetDemeaned(torch.utils.data.Dataset) :\n",
    "    def __init__(self, volumes):\n",
    "        self.path_volumes = volumes\n",
    "        \n",
    "        for i in range(len(self.path_volumes)):\n",
    "            vol_file = self.path_volumes[i]\n",
    "            volume = nib.load(vol_file).get_fdata()\n",
    "            new_img = volume.transpose(2, 0, 1)\n",
    "            \n",
    "            if i == 0: \n",
    "                self.imgs = new_img\n",
    "            else :\n",
    "                self.imgs = np.concatenate((self.imgs, new_img), axis=0)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c624ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets used for pretraining :\n",
      "ACDC\n",
      "n vol in train :  1\n",
      "n vol in validation :  50\n",
      "1\n",
      "Chaos\n",
      "n vol in train :  1\n",
      "n vol in validation :  5\n",
      "2\n",
      "HCP\n",
      "n vol in train :  1\n",
      "n vol in validation :  13\n",
      "3\n",
      "Medical Decathelon Prostate\n",
      "n vol in train :  1\n",
      "n vol in validation :  8\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "count_datasets = 0\n",
    "for config_dataset in config_datasets:\n",
    "    if config_dataset['experiment'] == 'pretraining' :\n",
    "        if count_datasets >= n_datasets :\n",
    "            break\n",
    "        count_datasets += 1\n",
    "assert count_datasets == n_datasets\n",
    "\n",
    "# Dataset Initialization\n",
    "datasets_train = []\n",
    "datasets_validation = []\n",
    "datasets_loaders_demeaned = []\n",
    "        \n",
    "print('Datasets used for pretraining :')\n",
    "\n",
    "count_datasets = 0\n",
    "for config_dataset in config_datasets:\n",
    "    \n",
    "    if config_dataset['experiment'] == 'pretraining' :\n",
    "        if count_datasets >= n_datasets :\n",
    "            break\n",
    "        count_datasets += 1\n",
    "        \n",
    "        print(config_dataset['Data'])\n",
    "        \n",
    "        current_datasets_train = []\n",
    "        current_datasets_validation = []\n",
    "        count = 0\n",
    "        for path in Path( config_dataset[\"savedir\"]).rglob( \"train/*/img.nii.gz\"):\n",
    "            if count > 0 :\n",
    "                break\n",
    "            current_datasets_train.append(path)\n",
    "            count += 1\n",
    "        for path in Path( config_dataset[\"savedir\"]).rglob( \"validation/*/img.nii.gz\"):\n",
    "            if count > 0 :\n",
    "                break\n",
    "            current_datasets_train.append(path)\n",
    "            count += 1\n",
    "        for path in Path( config_dataset[\"savedir\"]).rglob( \"test/*/img.nii.gz\"):\n",
    "            current_datasets_validation.append(path)\n",
    "            \n",
    "        print('n vol in train : ', len(current_datasets_train))\n",
    "        print('n vol in validation : ', len(current_datasets_validation))\n",
    "        datasets_train.append(current_datasets_train)\n",
    "        datasets_validation.append(current_datasets_validation)\n",
    "        \n",
    "        dataset_demeaned = PreTrainDatasetDemeaned(current_datasets_train)\n",
    "        datasets_loader_demeaned = DataLoader(dataset_demeaned,\n",
    "                            num_workers=1,\n",
    "                            batch_size = 32,\n",
    "                            pin_memory=True,\n",
    "                            shuffle=False,\n",
    "                            drop_last=False)\n",
    "        datasets_loaders_demeaned.append(datasets_loader_demeaned)\n",
    "        print(len(datasets_loaders_demeaned))\n",
    "        \n",
    "        \n",
    "\n",
    "trans = custom_transforms(config_encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ccd55e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets used for pretraining :\n",
      "ACDC\n",
      "n vol in train :  49\n",
      "n vol in validation :  50\n",
      "4\n",
      "Chaos\n",
      "n vol in train :  15\n",
      "n vol in validation :  5\n",
      "4\n",
      "HCP\n",
      "n vol in train :  36\n",
      "n vol in validation :  13\n",
      "4\n",
      "Medical Decathelon Prostate\n",
      "n vol in train :  24\n",
      "n vol in validation :  8\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "count_datasets = 0\n",
    "for config_dataset in config_datasets:\n",
    "    if config_dataset['experiment'] == 'pretraining' :\n",
    "        if count_datasets >= n_datasets :\n",
    "            break\n",
    "        count_datasets += 1\n",
    "assert count_datasets == n_datasets\n",
    "\n",
    "# Dataset Initialization\n",
    "datasets_train = []\n",
    "datasets_validation = []\n",
    "#datasets_loaders_demeaned = []\n",
    "        \n",
    "print('Datasets used for pretraining :')\n",
    "\n",
    "count_datasets = 0\n",
    "for config_dataset in config_datasets:\n",
    "    \n",
    "    if config_dataset['experiment'] == 'pretraining' :\n",
    "        if count_datasets >= n_datasets :\n",
    "            break\n",
    "        count_datasets += 1\n",
    "        \n",
    "        print(config_dataset['Data'])\n",
    "        \n",
    "        current_datasets_train = []\n",
    "        current_datasets_validation = []\n",
    "        \n",
    "        for path in Path( config_dataset[\"savedir\"]).rglob( \"train/*/img.nii.gz\"):\n",
    "            \n",
    "            current_datasets_train.append(path)\n",
    "            \n",
    "        for path in Path( config_dataset[\"savedir\"]).rglob( \"validation/*/img.nii.gz\"):\n",
    "            \n",
    "            current_datasets_train.append(path)\n",
    "            \n",
    "        for path in Path( config_dataset[\"savedir\"]).rglob( \"test/*/img.nii.gz\"):\n",
    "            current_datasets_validation.append(path)\n",
    "            \n",
    "        print('n vol in train : ', len(current_datasets_train))\n",
    "        print('n vol in validation : ', len(current_datasets_validation))\n",
    "        datasets_train.append(current_datasets_train)\n",
    "        datasets_validation.append(current_datasets_validation)\n",
    "        \n",
    "        dataset_demeaned = PreTrainDatasetDemeaned(current_datasets_train)\n",
    "        datasets_loader_demeaned = DataLoader(dataset_demeaned,\n",
    "                            num_workers=1,\n",
    "                            batch_size = 32,\n",
    "                            pin_memory=True,\n",
    "                            shuffle=False,\n",
    "                            drop_last=False)\n",
    "        #datasets_loaders_demeaned.append(datasets_loader_demeaned)\n",
    "        print(len(datasets_loaders_demeaned))\n",
    "        \n",
    "        \n",
    "dataset_train = PreTrainDataset(config_encoder, datasets_train)\n",
    "dataset_loader_train = DataLoader(dataset_train,\n",
    "                            num_workers=1,\n",
    "                            batch_size= n_volumes,\n",
    "                            pin_memory=True,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True)\n",
    "\n",
    "dataset_validation = PreTrainDataset(config_encoder, datasets_validation)\n",
    "dataset_loader_validation = DataLoader(dataset_validation,\n",
    "                            num_workers=1,\n",
    "                            batch_size=n_volumes,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True)\n",
    "\n",
    "trans = custom_transforms(config_encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d661bf4f",
   "metadata": {},
   "source": [
    "if loss_pretraining == 'demeaned_representation' :\n",
    "    # Calculate the mean representation of each datasets\n",
    "    all_volumes_per_dataset = []\n",
    "    for i_dataset in range(n_datasets) :\n",
    "\n",
    "        for count, path_vol in enumerate(datasets_train[i_dataset]) :\n",
    "            vol_file = path_vol\n",
    "            volume = torch.tensor(nib.load(vol_file).get_fdata().transpose(2, 0, 1)).view((-1, n_channels, *resize_size))\n",
    "\n",
    "            if count == 0 :\n",
    "                volumes = volume\n",
    "            else :\n",
    "                volumes = torch.cat([volumes, volume])\n",
    "        print(volumes.shape)\n",
    "        all_volumes_per_dataset.append(volumes)\n",
    "print(len(all_volumes_per_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9289f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "670cf7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Train...\n",
      "Epoch 000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:12<00:00,  9.05s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train loss: 10.970399\n",
      "Doing Train...\n",
      "Epoch 001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.03it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n",
      "100%|██████████| 8/8 [01:12<00:00,  9.09s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train loss: 10.624954\n",
      "Doing Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current validation loss: 9.993177\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "losses = pd.DataFrame(columns = ['epoch', 'train loss', 'validation loss'])\n",
    "\n",
    "# Training\n",
    "for epoch in range(max_epochs) :\n",
    "    print(\"Doing Train...\")\n",
    "    print(\"Epoch {:03d}\".format(epoch))\n",
    "\n",
    "    model.train()\n",
    "    batch_train_loss = []\n",
    "    \n",
    "    # Calculate mean representation of each dataset from current model\n",
    "    if loss_pretraining == 'demeaned_representation_loss' or loss_pretraining == 'demeaned_representation_loss_2' :\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            mean_representations = []\n",
    "            for i_dataset in range(n_datasets):\n",
    "                if epoch == 0 :\n",
    "                    mean_representation = torch.zeros((128, 6, 6))\n",
    "                else :\n",
    "                    for id_batch, batch_x in enumerate(tqdm(datasets_loaders_demeaned[i_dataset])):\n",
    "                        batch = batch_x.float().to(device)\n",
    "                        batch = batch.view((-1, n_channels, *resize_size))\n",
    "                        if id_batch == 0 :\n",
    "                            pred_representation = model.enc(batch).squeeze()\n",
    "                        else :\n",
    "                            pred_representation = torch.cat([pred_representation, model.enc(batch).squeeze()])\n",
    "                    mean_representation = torch.mean(pred_representation, dim = 0).to(device)\n",
    "                mean_representations.append(mean_representation)\n",
    "          \n",
    "    \n",
    "    for id_batch, batch_x in enumerate(tqdm(dataset_loader_train)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch = batch_x.float().to(device)\n",
    "        batch = batch.view((-1, n_channels, *resize_size))\n",
    "        train_batch = batch\n",
    "        \n",
    "        \n",
    "        if loss_pretraining == 'demeaned_representation_loss' :\n",
    "            # similar as representation_loss but with demeaned version loss\n",
    "            # subtract the mean of the representation of the repectif dataset of last epoch to compute the loss\n",
    "            for i in range(n_transforms) :\n",
    "                train_batch = torch.cat([train_batch, trans(batch, dtrans ='option_2')])\n",
    "            pred = model(train_batch).squeeze()\n",
    "            pred_representation = model.enc(train_batch).squeeze()\n",
    "            \n",
    "            loss_1 = criterion(pred)\n",
    "            loss_2 = criterion2(pred_representation, mean_representations)\n",
    "            \n",
    "            loss = lambda_ * loss_1 + (1 - lambda_) * loss_2\n",
    "            \n",
    "        elif loss_pretraining == 'demeaned_representation_loss_2' :\n",
    "            for i in range(n_transforms) :\n",
    "                train_batch = torch.cat([train_batch, trans(batch, dtrans ='option_2')])\n",
    "            pred_representation = model.enc(train_batch).squeeze()\n",
    "            \n",
    "            # Demeaned reg_pred with respective mean of the dataset \n",
    "            pred_demeaned_representation = pred_representation.clone()\n",
    "            for i in range(pred_representation.shape[0]) :\n",
    "                pred_demeaned_representation[i,:,:,:] -=  mean_representations[((i // n_parts) % n_datasets)] \n",
    "                #print((i // n_parts) % n_datasets)\n",
    "            pred = model.g1(pred_demeaned_representation).squeeze()\n",
    "            \n",
    "            loss = criterion(pred)\n",
    "            \n",
    "        if id_batch == 0 :\n",
    "            pred_train = pred\n",
    "        else :\n",
    "            pred_train = torch.cat([pred_train, pred.detach()])\n",
    "        \n",
    "        batch_train_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 2 == 0 :\n",
    "        directory_predictions = str(save_directory) + \"/predictions/\" + str(epoch) +\"/\" \n",
    "        Path(directory_predictions).mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(pred_train, directory_predictions+ \"pred_train.pt\")\n",
    "        \n",
    "    train_loss = statistics.mean(batch_train_loss)\n",
    "    print(\"Current train loss: %f\" % train_loss)  \n",
    "    \n",
    "    if (epoch+1) % 2 == 0 :\n",
    "        \n",
    "        # Validation \n",
    "        batch_val_loss = []\n",
    "\n",
    "        print(\"Doing Validation...\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = pd.DataFrame(columns = ['type', 'prediction'])\n",
    "\n",
    "            for id_batch, batch_x in enumerate(tqdm(dataset_loader_validation)):\n",
    "\n",
    "                batch = batch_x.float().to(device)\n",
    "                batch = batch.view((-1, n_channels, *resize_size))\n",
    "                val_batch = batch\n",
    "\n",
    "                \n",
    "                if loss_pretraining == 'demeaned_representation_loss' :\n",
    "                    for i in range(n_transforms) :\n",
    "                        val_batch = torch.cat([val_batch, trans(batch, dtrans ='option_2')])\n",
    "                        \n",
    "                    #val_batch = torch.cat([val_batch, trans(batch, dtrans ='option_2')])\n",
    "                    pred_representation = model.enc(val_batch).squeeze()\n",
    "                    pred = model(val_batch).squeeze()\n",
    "                    \n",
    "                    loss_1 = criterion(pred)\n",
    "                    loss_2 = criterion2(pred_representation, mean_representations)\n",
    "                    \n",
    "                    loss = lambda_ * loss_1 + (1 - lambda_) * loss_2\n",
    "                    \n",
    "                elif loss_pretraining == 'demeaned_representation_loss_2' :\n",
    "                    for i in range(n_transforms) :\n",
    "                        val_batch = torch.cat([val_batch, trans(batch, dtrans ='option_2')])\n",
    "                    pred_representation = model.enc(val_batch).squeeze()\n",
    "\n",
    "                    # Demeaned reg_pred with respective mean of the dataset \n",
    "                    pred_demeaned_representation = pred_representation.clone()\n",
    "                    for i in range(pred_representation.shape[0]) :\n",
    "                        pred_demeaned_representation[i,:,:,:] -=  mean_representations[((i // n_parts) % n_datasets)] \n",
    "                        #print((i // n_parts) % n_datasets)\n",
    "                    pred = model.g1(pred_demeaned_representation).squeeze()\n",
    "\n",
    "                    loss = criterion(pred)\n",
    "                    \n",
    "                if id_batch == 0 :\n",
    "                    pred_validation = pred\n",
    "                else :\n",
    "                    pred_validation = torch.cat([pred_validation, pred])\n",
    "\n",
    "                batch_val_loss.append(loss.item())\n",
    "\n",
    "        torch.save(pred_validation, directory_predictions+\"pred_validation.pt\")\n",
    "    \n",
    "        validation_loss = statistics.mean(batch_val_loss)\n",
    "\n",
    "        print(\"Current validation loss: %f\" % validation_loss)  \n",
    "    \n",
    "    if (epoch+1) % 2 == 0 :\n",
    "        losses = losses.append([{'epoch': epoch, 'train loss' : train_loss, 'validation loss' : validation_loss}])\n",
    "        steps += 1\n",
    "    else : \n",
    "        losses = losses.append([{'epoch': epoch, 'train loss' : train_loss}])\n",
    "        steps += 1\n",
    "        \n",
    "    # Save loss and model at each 50 epoch\n",
    "    if (epoch+1) % 2 == 0 :\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                },str(save_models) + \"checkpoints_\" + str(epoch)+\".pt\")\n",
    "\n",
    "        losses.to_pickle(str(save_directory) + \"/losses.pkl\")\n",
    "    \n",
    "losses.to_pickle(str(save_directory) + \"/losses.pkl\")\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edfa50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
