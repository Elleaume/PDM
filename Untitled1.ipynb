{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303d4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "../img_cropped/abide/test/subject_33260/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33265/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33266/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33258/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33287/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33278/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33262/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33279/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33268/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33277/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33290/img.nii.gz\n",
      "../img_cropped/abide/test/subject_33269/img.nii.gz\n",
      "pretraining :  baseline\n",
      "run :  0\n",
      "n train :  1\n",
      "trained_models/Abide/baseline/batch_size_128/crossentropy_loss_lr_0.001_1_vol_in_train_2_vol_in_val/run_0/save_models/checkpoints_best_model1139.pt\n",
      "Running model seg_unet\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nibabel as nib\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from decoder_pretrain import DecoderPretrainNet\n",
    "from encoder_pretrain import EncoderPretrainNet\n",
    "from gloss_dminus import GlobalLossDminus\n",
    "from gloss_d import GlobalLossD\n",
    "from dice_loss import DiceLoss\n",
    "from seg_unet import UNet_pretrained, UNet\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import json\n",
    "import statistics\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from training_utils import *\n",
    "from data_augmentation_utils import DataAugmentation\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with open('configs/preprocessing_datasets.json') as config_file:\n",
    "    config_datasets = json.load(config_file)\n",
    "with open('configs/seg_unet_Abide.json') as config_file:\n",
    "    config_seg = json.load(config_file)\n",
    "with open('configs/config_encoder.json') as config_file:\n",
    "    config_encoder = json.load(config_file)\n",
    "\n",
    "#n_vol_train = config_seg['n_vol_train']\n",
    "n_vol_val = config_seg['n_vol_val']\n",
    "\n",
    "lambda_ = 0.6\n",
    "\n",
    "n_vol_trains = [1,2,5]\n",
    "seeds = [0, 10, 20, 30, 40, 50] \n",
    "pretrainings= ['baseline',\n",
    "                'pretrained with Abide (global_d)',\n",
    "                'pretrained with Abide (global_dminus)',\n",
    "                'pretrained with HCP (global_d)',\n",
    "                'pretrained with HCP (global_dminus)',\n",
    "                'pretrained with HCP-Abide (global_d)',\n",
    "                'pretrained with HCP-Abide (global_dminus)',\n",
    "                'pretrained with ACDC-Abide-Chaos-MedDecath Prostate (global_d)',\n",
    "                'pretrained with ACDC-Abide-Chaos-MedDecath Prostate (global_dminus)',\n",
    "                'pretrained with MMWHS-HCP-Chaos-MedDecath Prostate (global_d)',\n",
    "                'pretrained with MMWHS-HCP-Chaos-MedDecath Prostate (global_dminus)',\n",
    "                'pretrained with ACDC-HCP-Chaos-MedDecath Prostate (global_d)',\n",
    "                'pretrained with ACDC-HCP-Chaos-MedDecath Prostate (global_dminus)'\n",
    "]\n",
    "loss_unet = config_seg['loss_unet']\n",
    "\n",
    "dataset = config_seg['dataset']\n",
    "resize_size = config_seg['resize_size']\n",
    "n_channels = config_seg['n_channels']\n",
    "max_epochs = config_seg['max_epochs']\n",
    "max_steps = config_seg['max_steps']\n",
    "n_classes = config_seg['n_classes']\n",
    "batch_size = config_seg['batch_size']\n",
    "lr = config_seg['lr']\n",
    "weight_pretrained = config_seg['weight_pretrained']\n",
    "\n",
    "save_global_path = config_seg['save_global_path']\n",
    "\n",
    "\n",
    "if dataset == 'Abide':\n",
    "    n_classes = 15\n",
    "    lab = [1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "    weights = torch.tensor([0.025, 0.075, 0.075, 0.035, 0.035, 0.075, 0.075, 0.075, \n",
    "                                   0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075], dtype=torch.float32)\n",
    "    \n",
    "    total_n_volumes = 24\n",
    "    n_vol_test = 12\n",
    "    \n",
    "elif dataset == 'CIMAS' or dataset == 'ACDC' :\n",
    "    n_classes = 4\n",
    "    lab = [1,2,3]\n",
    "    weights = torch.tensor([0.1, 0.3, 0.3, 0.3], dtype=torch.float32)\n",
    "    \n",
    "    if dataset == 'CIMAS' :\n",
    "        total_n_volumes = 13\n",
    "        n_vol_test = 7\n",
    "    else :\n",
    "        total_n_volumes = 69\n",
    "        n_vol_test = 30\n",
    "        \n",
    "if dataset == 'Metastases':\n",
    "    n_classes = 2\n",
    "    lab = [1]\n",
    "    weights = torch.tensor([0.1, 0.9], dtype=torch.float32)\n",
    "    \n",
    "    total_n_volumes = 53\n",
    "    n_vol_test = 24\n",
    "    n_vol_val = 18\n",
    "        \n",
    "def test_loading(config_datasets, config_seg, dataset, total_n_volumes=24, n_volumes=2, \n",
    "                       split_set = 'train',shuffle = False, idx_vols_val = None, return_idx = False):\n",
    "    img_dataset = []\n",
    "    mask_dataset = []\n",
    "        \n",
    "    idx_vols = select_random_volumes(total_n_volumes, n_volumes) \n",
    "    if idx_vols_val != None :\n",
    "        assert len(idx_vols_val) + n_volumes <= total_n_volumes\n",
    "        while any(item in idx_vols for item in idx_vols_val) :\n",
    "            idx_vols = select_random_volumes(total_n_volumes, n_volumes) \n",
    "            \n",
    "    count = -1\n",
    "    \n",
    "    for config_dataset in config_datasets :\n",
    "        if config_dataset['Data'] == dataset :  \n",
    "            for path in Path(config_dataset['savedir']+ split_set +'/').rglob('subject_*/'):\n",
    "                \n",
    "                # We want total path not individual path of images\n",
    "                if \"nii.gz\" in str(path) or \".png\" in str(path) :\n",
    "                    continue\n",
    "                count += 1   \n",
    "                \n",
    "                # Different criterion to stop adding train or test volumes\n",
    "                if split_set == 'test':\n",
    "                    if count >= n_volumes :\n",
    "                        break\n",
    "                else :\n",
    "                    if count not in idx_vols :\n",
    "                        continue\n",
    "                        \n",
    "                # Add the image and the corresponding mask to the datasets\n",
    "                for path_image in path.rglob(\"img.nii.gz\") :\n",
    "                    img_dataset.append(path_image)\n",
    "                    print(path_image)\n",
    "                for path_mask in path.rglob(\"mask.nii.gz\") :\n",
    "                    mask_dataset.append(path_mask)\n",
    "    \n",
    "    return img_dataset, mask_dataset\n",
    "\n",
    "print('Test set')\n",
    "img_dataset, mask_dataset = test_loading(config_datasets, config_seg, dataset, total_n_volumes=n_vol_test, \n",
    "                                        n_volumes=n_vol_test, split_set = 'test', \n",
    "                                        shuffle = False)\n",
    "\n",
    "\n",
    "for pretraining in pretrainings :\n",
    "    print('pretraining : ',pretraining)\n",
    "    for run, seed in enumerate(seeds) :\n",
    "        print('run : ', run)\n",
    "        for n_vol_train in n_vol_trains :\n",
    "            print('n train : ', n_vol_train)\n",
    "            # Load best trained model\n",
    "            save_models_path = save_global_path + dataset + '/' + pretraining + '/batch_size_' + str(batch_size)  + \\\n",
    "                    '/'+ loss_unet +'_lr_'+ str(lr) + '_' + str(n_vol_train) + '_vol_in_train_'  + str(n_vol_val)  + \\\n",
    "                     '_vol_in_val' + '/run_' + str(run)\n",
    "            best_model = UNet(config_seg)\n",
    "            for path in Path(str(save_models_path) + '/save_models/').rglob('checkpoints_best_model*.pt'):\n",
    "                print(path)\n",
    "                checkpoint = torch.load(path, map_location=torch.device('cpu') )\n",
    "                best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "            best_model.to(device)\n",
    "            print(\"Running model %s\" % config_seg[\"model\"])\n",
    "            \n",
    "#             print('Using : ')\n",
    "#             print(pretraining)\n",
    "#             print('dataset : ',dataset)\n",
    "#             print('max_epochs : ', max_epochs)\n",
    "#             print('max_steps : ', max_steps)\n",
    "#             print('batch_size : ', batch_size)\n",
    "#             print('n_vol_train : ', n_vol_train)\n",
    "#             print('n_vol_val : ', n_vol_val)\n",
    "#             print('n_vol_test : ', n_vol_test)\n",
    "#             print('loss unet : ', loss_unet)\n",
    "#             print('lr : ', lr)\n",
    "#             print('save_models_path : ', save_models_path)\n",
    "#             print('current run : ', str(run) , ' with seed : ',  str(seed))\n",
    "#             print(\"Using device: \", device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                best_model.eval()\n",
    "\n",
    "                #print(\"Epoch {:03d}\".format(best_epoch))\n",
    "                batch_test_loss = []\n",
    "                f1_mean = []\n",
    "                f1_arr = []\n",
    "                for i in range(len(img_dataset)) :\n",
    "                    #if str(img_dataset[i])  == '../img_cropped/abide/test/subject_'+str(subj_label)+'/img.nii.gz':\n",
    "                    #print(img_dataset[i])\n",
    "                    vol_file= img_dataset[i]\n",
    "                    mask_file = mask_dataset[i]\n",
    "\n",
    "                    volume_data = nib.load(vol_file)\n",
    "                    mask_data = nib.load(mask_file)\n",
    "\n",
    "                    affine_volume = volume_data.affine\n",
    "                    affine_mask = mask_data.affine\n",
    "\n",
    "                    volume = volume_data.get_fdata()\n",
    "                    mask = mask_data.get_fdata()\n",
    "\n",
    "                    assert volume.shape == mask.shape\n",
    "                    batch_x = torch.from_numpy(volume.transpose(2, 0, 1))\n",
    "                    batch_y = torch.from_numpy(mask.transpose(2, 0, 1))\n",
    "                    \n",
    "                    test_batch_x = (batch_x.float().to(device)).view((-1, n_channels, *resize_size))\n",
    "                    test_batch_y = (batch_y.long().to(device)).view((-1, n_channels, *resize_size))\n",
    "\n",
    "                    pred = best_model(test_batch_x)\n",
    "                    pred_labels_baseline = F.softmax(pred, dim=1) \n",
    "                    pred_labels_baseline = torch.argmax(pred_labels_baseline, dim=1)\n",
    "\n",
    "                    f1_val = compute_f1(test_batch_y.cpu(), pred_labels_baseline.cpu(), lab)\n",
    "                    f1_mean.append(np.mean(f1_val))\n",
    "                    f1_arr.append(f1_val)\n",
    "                    baseline_mean_score = np.mean(f1_val)\n",
    "                    #print('f1 scores : ', f1_val)\n",
    "                    #print('mean dice score : ', baseline_mean_score )\n",
    "\n",
    "                    if loss_unet == 'crossentropy_loss' :\n",
    "                        criterion = nn.CrossEntropyLoss(weights.to(device))\n",
    "                        loss = criterion(pred.float(), test_batch_y.squeeze(1))\n",
    "                    elif loss_unet == 'dice_loss' :\n",
    "                        criterion = DiceLoss()\n",
    "                        loss = criterion(pred, test_batch_y, device)\n",
    "                    elif loss_unet == 'mixed_dice_crossentropy_loss' :\n",
    "                        criterion = nn.CrossEntropyLoss(weights.to(device))\n",
    "                        crossentropy_loss = criterion(pred.float(), test_batch_y.squeeze(1)).cpu()\n",
    "                        criterion = DiceLoss()\n",
    "                        dice_loss = criterion(pred, test_batch_y, device).to(device)\n",
    "                    else :\n",
    "                        print('ERROOOOR')\n",
    "\n",
    "                    batch_test_loss.append(loss.item())\n",
    "\n",
    "                test_loss = statistics.mean(batch_test_loss)\n",
    "                f1_mean_epoch = np.mean(f1_mean)\n",
    "                f1_arr_epoch = np.asarray(f1_arr)\n",
    "                #print('F1 score on test set : ', f1_mean_epoch)\n",
    "\n",
    "            infile = open(save_models_path+ '/results.pkl','rb')\n",
    "            results_BN = pickle.load(infile)\n",
    "#             print('test F1 before : ', results_BN['test F1'])\n",
    "\n",
    "            results = pd.DataFrame(columns = ['model', 'n vol train', 'lr', 'loss unet',\n",
    "                                                              'train F1', 'validation F1', 'test F1', \n",
    "                                                              'train loss', 'validation loss', 'test loss',\n",
    "                                                              'best epoch'])\n",
    "\n",
    "            results = results.append([{'model': pretraining, 'n vol train': n_vol_train, 'lr' : results_BN['lr'], \n",
    "                               'loss unet' : loss_unet,\n",
    "                               'batch_size' : batch_size,\n",
    "                               'train F1': results_BN['train F1'][0], \n",
    "                               'validation F1' : results_BN['validation F1'][0], \n",
    "                               'test F1' : f1_mean_epoch, \n",
    "                               'train loss' : results_BN['train loss'][0], \n",
    "                               'validation loss' : results_BN['validation loss'][0], \n",
    "                               'test loss' : test_loss, \n",
    "                               'best epoch' : results_BN['best epoch'][0], \n",
    "                               'f1_arr' : np.mean(f1_arr_epoch, axis = 0)\n",
    "                               }])\n",
    "#             print('test F1 after : ', results['test F1'])\n",
    "\n",
    "            results.to_pickle(save_models_path + \"/results_2.pkl\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b35d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
